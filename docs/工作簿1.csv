"Matrix Compression Techniques									",,,,,,,,
Categoreis,Numerical,,,,,,Combination,
Paper,ALBERT,FWSVD,TFWSVD,Shapeshifter,KnGPT2,TensorGPT,LPAF,LoRAPrune
Features,Apply SVD after the word embedding layer,Use Fisher information to measure the weights importance,Improve the usage of Fisher information ,Decompose weight matrices with Kronecker product,Use Kronecker decomposition for compression of the GPT model.,Treate token embeddings as matrix product states,Combine SVD with Unstructured Pruning to compress PLM,Use LoRA to instruct the UP process
"Pros									",Untying the WordPiece embedding size E from the hidden layer size H.,"Align the optimization objective of compression with the model, rather than just minimizing reconstruction error.",Use element-wise Fisher information instead of sharing in one row,The output of a sum of Kronecker products layer is not limited to r-dimensional subspace;,Combine Kronecker with KD,Flexible vocabulary and distribute-computation friendly,"UP makes matrices too be low-rank, SVD enable the UP results to run on general devices",Reduces computational resources required for pruning.
"Cons									",Only available for embedding layers,Task specific; Fisher information is shared through certain rows; Refine-tuning is required,"More training steps, 1.5x of FWSVD",The reshaping OP caused 10% more time in training and inference; No implementation code.,"As an ACL findings, experiments and conclusions are not sufficient",To be eplored on more structures of LLMs,The UP strategy is based on rows instead of elements; Refine-tuing required,Hardware support is required
"Compression ratio									",10%,40%(transformer blocks only),40%(transformer blocks only),80% ~ 90%,33%,33%,84%,90%
Capacities retained,,70% / 97%,80% / 99%,95%,95% / 99%,,95%,90%
